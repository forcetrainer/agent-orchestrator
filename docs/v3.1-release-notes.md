# System Prompt v3.1 Release Notes

**Date**: 2025-10-08
**Version**: v3.1 - Action Tag Execution Pattern
**Status**: Ready for Testing

---

## Summary

Version 3.1 addresses a critical behavioral difference between Claude Code and the app: how `<action>` tags are interpreted and executed. The app was delegating tasks to the user rather than performing analysis itself.

## The Problem

**Observation**: Casey deep-dive Step 2 produced fundamentally different outputs:

**Claude Code** ‚úÖ:
```
Based on Alex's intake, I can see the core issue is about RCA completion delays.
Here's what I'm seeing from the intake:
- Problem: RCAs not completed on time ‚Üí delays mitigations
- Goal: Complete RCAs within 5 days
- Solution: Automated reminders + escalations

For the refined problem statement, you might want to ask John:
- "Can you describe a recent example where delayed RCA led to repeat outage?"
- "What's preventing RCAs from being completed on time?"
...
```

**App with v3.0** ‚ùå:
```
You lead: please provide a refined problem statement with business context
based on the attached intake. Ask John to confirm or adjust wording.
```

**Analysis**: The instruction `<action>Create refined summary focused on ITSM enhancement needs</action>` was being interpreted as "tell the user to create a summary" rather than "I (the agent) should create a summary."

## Root Cause

After analyzing all workflow instruction files across agents (Alex, Casey, Pixel), we identified that `<action>` tags follow THREE distinct patterns:

1. **Agent Execution Actions** - Agent performs the action
   - Verbs: "Create", "Compile", "Identify", "Present", "Based on", "Read and summarize"
   - Example: `<action>Create refined summary</action>`
   - Expected: Agent analyzes context ‚Üí presents findings ‚Üí asks for validation

2. **Guidance/Suggestion Actions** - Agent provides examples
   - Pattern: "Suggest questions about X:", "Suggest to [user]:", "Prompt for:"
   - Example: `<action>Suggest questions about ITSM module:</action>`
   - Expected: Agent offers example questions for user to use with stakeholders

3. **Meta/Conversational Actions** - Agent says this
   - Verbs: "Explain", "Shift to", "Acknowledge", "Thank the user"
   - Example: `<action>Shift to business value conversation</action>`
   - Expected: Agent uses this for conversational transitions

The app was treating **Type 1 (Agent Execution)** as **Type 3 (Meta/Instructional)**, leading to delegation instead of execution.

## The Solution

Added a new section to the system prompt: **INTERPRETING <action> TAGS - CRITICAL EXECUTION RULES**

This section:
1. Explicitly states that `<action>` tells the AGENT what to do, not what to tell the user
2. Provides pattern recognition for all three action types
3. Shows concrete examples with ‚úÖ CORRECT vs ‚ùå WRONG responses
4. Emphasizes: **ANALYZE ‚Üí PRESENT FINDINGS ‚Üí THEN ASK** (not DELEGATE ‚Üí ASK)

### Example from v3.1 Prompt:

```markdown
1. AGENT EXECUTION ACTIONS (you perform the action):
   If <action> starts with verbs like: "Create", "Compile", "Identify", "Present"
   ‚Üí YOU execute this action using loaded context
   ‚Üí Show your work/analysis to the user
   ‚Üí THEN ask for their input/validation

   Example:
   <action>Create refined summary focused on ITSM enhancement needs</action>

   ‚úÖ CORRECT Response:
   "Based on Alex's intake, here's what I'm seeing:
   - Problem: RCAs not completed on time
   - Goal: Complete within 5 days

   For the refined problem statement, you might want to ask John:
   - [contextual questions]"

   ‚ùå WRONG Response:
   "You lead: please provide a refined problem statement..."

   The Pattern: ANALYZE ‚Üí PRESENT FINDINGS ‚Üí THEN ASK
   NOT: DELEGATE ‚Üí ASK
```

## Key Changes

**File**: `lib/agents/prompts/system-prompt.md`

**New Section**: INTERPRETING <action> TAGS - CRITICAL EXECUTION RULES (~60 lines)

**Additions**:
- Pattern recognition for three action types with examples
- Explicit "you vs user" distinction
- Concrete examples showing correct vs incorrect interpretation
- Added to GENERAL CONVERSATIONAL RULES: "Act as a collaborative partner, not a passive question-asker"
- Added to USING LOADED CONTEXT: "Organize context into digestible bullet points or structured summaries"

## Expected Behavior Changes

### Before v3.1:
- Agent delegates tasks: "You lead: please provide..."
- No analysis shown
- Generic, transactional responses
- User left to do the thinking

### After v3.1:
- Agent performs analysis: "Here's what I'm seeing from the intake..."
- Shows organized findings (bullet points)
- Provides contextual guidance
- Collaborative partner approach
- Pattern: ANALYZE ‚Üí PRESENT ‚Üí THEN ASK

## Testing

### Validation Script
```bash
npx tsx scripts/test-system-prompt.ts
```

Expected output:
```
‚úÖ v3.1 Action Execution Pattern features detected
‚úÖ All checks passed!
```

### Behavioral Test Scenario

**Setup**: Casey deep-dive workflow, Step 2 (Refine problem statement)

**Prerequisites**:
1. Load Casey agent
2. Attach intake file from Alex (e.g., RCA completion issue)
3. Invoke deep-dive-itsm workflow

**Step 2 Check**:
- ‚úÖ Shows "Step 2: Refine problem statement"
- ‚úÖ Demonstrates it read the intake (shows specific details)
- ‚úÖ Organizes information into bullet points
- ‚úÖ Provides contextual example questions based on actual content
- ‚úÖ THEN asks for refined problem statement input
- ‚ùå Should NOT just say "you lead: please provide..."

### Comparison Test

Run same scenario in:
1. Claude Code (baseline reference)
2. App with v3.0 (should show delegation behavior)
3. App with v3.1 (should match Claude Code pattern)

Compare:
- Does agent show analysis before asking?
- Are questions contextual vs generic?
- Does it feel collaborative vs transactional?

## Migration Path

v3.1 is a DROP-IN REPLACEMENT for v3.0. No code changes required beyond the template update.

**To Deploy**:
1. ‚úÖ Template already updated: `lib/agents/prompts/system-prompt.md`
2. ‚úÖ systemPromptBuilder.ts already loads from template
3. ‚úÖ Test script updated to check v3.1 features
4. üîÑ Restart dev server to pick up new template
5. üîÑ Test with Casey deep-dive workflow

**To Rollback** (if needed):
```bash
cp lib/agents/prompts/versions/v3.0-context-aware.md \
   lib/agents/prompts/system-prompt.md
```

## Files Modified

```
lib/agents/prompts/
‚îú‚îÄ‚îÄ CHANGELOG.md                          # Added v3.1 entry
‚îú‚îÄ‚îÄ README.md                             # Updated current version
‚îú‚îÄ‚îÄ system-prompt.md                      # Updated to v3.1
‚îî‚îÄ‚îÄ versions/
    ‚îî‚îÄ‚îÄ v3.1-action-execution.md          # New version snapshot

lib/agents/
‚îî‚îÄ‚îÄ systemPromptBuilder.ts                # Updated version comments

scripts/
‚îî‚îÄ‚îÄ test-system-prompt.ts                 # Updated to detect v3.1

docs/
‚îú‚îÄ‚îÄ system-prompt-versioning.md           # Updated with v3.1 info
‚îî‚îÄ‚îÄ v3.1-release-notes.md                 # This file
```

## Success Criteria

v3.1 is successful if:

1. ‚úÖ Test script passes (validates template loads with v3.1 features)
2. üîÑ Casey Step 2 shows analysis before asking (not delegation)
3. üîÑ Agent references specific details from loaded intake files
4. üîÑ Questions are contextual, not generic
5. üîÑ Behavior closely matches Claude Code's collaborative approach

## Next Steps

1. **Test**: Run Casey deep-dive with RCA intake scenario
2. **Compare**: Side-by-side with Claude Code output
3. **Document**: Fill in "Actual Result" in CHANGELOG.md
4. **Iterate**: If still not matching, create v3.2 with further refinements

## Notes

- This is iteration #3 in reverse-engineering Claude Code's behavior
- We're getting closer but may need more refinements
- Each version is tracked with problem ‚Üí solution ‚Üí result cycle
- Version history preserved for learning and rollback capability

---

**Questions?** See:
- Full details: `lib/agents/prompts/CHANGELOG.md`
- Template: `lib/agents/prompts/system-prompt.md`
- Implementation docs: `docs/system-prompt-versioning.md`
