# Server-Sent Events (SSE) Format Documentation

**Story 6.8: Visual Response Streaming Display**

This document describes the Server-Sent Events format used for streaming agent responses from backend to frontend.

## Overview

The `/api/chat` endpoint returns a streaming response using the SSE protocol. This allows the frontend to display agent responses token-by-token as they are generated by the LLM, while preserving the existing agentic execution loop (tool calls, workflow loading, etc.).

## SSE Protocol Basics

- **Content-Type:** `text/event-stream`
- **Format:** Each event is prefixed with `data:` and terminated with `\n\n`
- **Connection:** Keep-alive, no caching

### Example SSE Stream

```
data: {"type":"token","content":"Hello"}

data: {"type":"token","content":" world"}

data: {"type":"status","message":"Reading workflow.md..."}

data: {"type":"token","content":"!"}

data: {"type":"conversationId","conversationId":"abc-123"}

data: [DONE]

```

## Event Types

### 1. Token Event

Emitted when the LLM generates text content. Sent progressively as tokens arrive from OpenAI.

**Format:**
```json
{
  "type": "token",
  "content": "string"
}
```

**Example:**
```
data: {"type":"token","content":"The"}

data: {"type":"token","content":" quick"}

data: {"type":"token","content":" brown"}

```

**Frontend Handling:**
- Accumulate `content` into streaming buffer
- Render progressively in MessageBubble component
- Batched for 16ms (1 frame at 60fps) before React update

---

### 2. Status Event

Emitted during tool execution to show user what the agent is doing.

**Format:**
```json
{
  "type": "status",
  "message": "string"
}
```

**Examples:**
```
data: {"type":"status","message":"Agent is thinking"}

data: {"type":"status","message":"Reading workflow.md..."}

data: {"type":"status","message":"Writing output.txt..."}

data: {"type":"status","message":"Executing brainstorming workflow..."}

```

**Frontend Handling:**
- Display in status indicator below input
- Replace previous status message
- Cleared when streaming completes

---

### 3. ConversationId Event

Emitted before `[DONE]` to return the conversation ID for multi-turn context.

**Format:**
```json
{
  "type": "conversationId",
  "conversationId": "string"
}
```

**Example:**
```
data: {"type":"conversationId","conversationId":"conv_1728394756123_agent-dev"}

```

**Frontend Handling:**
- Store conversationId for next message
- Required for multi-turn conversations (preserves message history)

---

### 4. Error Event

Emitted when an error occurs during streaming (API error, tool execution failure, timeout, etc.).

**Format:**
```json
{
  "type": "error",
  "message": "string"
}
```

**Examples:**
```
data: {"type":"error","message":"OpenAI API rate limit exceeded"}

data: {"type":"error","message":"Tool execution failed: File not found"}

data: {"type":"error","message":"Maximum iterations exceeded"}

```

**Frontend Handling:**
- Display error message to user
- Halt streaming
- Clear streaming state

---

### 5. Completion Event

Signals the end of the stream. Always the last event sent.

**Format:**
```
data: [DONE]

```

**Frontend Handling:**
- Mark streaming as complete
- Return `finalContent` to ChatPanel
- ChatPanel adds final message to message history

---

## Flow Diagram

```
┌──────────────────────────────────────────────────────────────┐
│ User sends message → /api/chat                               │
└──────────────────────────────────────────────────────────────┘
                            ↓
┌──────────────────────────────────────────────────────────────┐
│ Backend: Build system prompt, critical context               │
└──────────────────────────────────────────────────────────────┘
                            ↓
┌──────────────────────────────────────────────────────────────┐
│ Backend: Call OpenAI with stream: true                       │
└──────────────────────────────────────────────────────────────┘
                            ↓
           ┌────────────────┴────────────────┐
           │                                  │
┌──────────▼──────────┐          ┌───────────▼──────────┐
│ delta.content       │          │ delta.tool_calls     │
│ → Emit token event  │          │ → PAUSE streaming    │
└─────────────────────┘          └──────────────────────┘
           │                                  │
           │                      ┌───────────▼──────────┐
           │                      │ Execute tool         │
           │                      └──────────────────────┘
           │                                  │
           │                      ┌───────────▼──────────┐
           │                      │ Emit status event    │
           │                      └──────────────────────┘
           │                                  │
           │                      ┌───────────▼──────────┐
           │                      │ Inject result        │
           │                      │ → RESUME streaming   │
           │                      └──────────────────────┘
           │                                  │
           └──────────────┬───────────────────┘
                          ↓
           ┌──────────────────────────────────┐
           │ finish_reason present            │
           │ → Emit conversationId event      │
           │ → Emit [DONE] event              │
           │ → Close stream                   │
           └──────────────────────────────────┘
```

## Backend Implementation

**Location:** `app/api/chat/route.ts`

**Key Code:**
```typescript
const stream = new ReadableStream({
  async start(controller) {
    const encoder = new TextEncoder();

    // Agentic loop with streaming
    while (iterations < MAX_ITERATIONS) {
      const response = await client.chat.completions.create({
        model,
        messages: completeMessages,
        tools,
        stream: true, // Enable streaming
      });

      for await (const chunk of response) {
        const delta = chunk.choices[0]?.delta;

        // Stream tokens
        if (delta?.content) {
          controller.enqueue(
            encoder.encode(`data: ${JSON.stringify({ type: 'token', content: delta.content })}\n\n`)
          );
        }

        // Pause for tool calls
        if (delta?.tool_calls) {
          // Emit status
          controller.enqueue(
            encoder.encode(`data: ${JSON.stringify({ type: 'status', message: 'Reading X...' })}\n\n`)
          );

          // Execute tool (blocking)
          const result = await executeToolCall(toolCall, pathContext);

          // Inject result into context
          completeMessages.push({ role: 'tool', content: result });

          // Resume streaming in next iteration
          continue;
        }
      }

      // No tool calls → complete
      controller.enqueue(
        encoder.encode(`data: ${JSON.stringify({ type: 'conversationId', conversationId: 'xxx' })}\n\n`)
      );
      controller.enqueue(encoder.encode('data: [DONE]\n\n'));
      controller.close();
      return;
    }
  }
});

return new Response(stream, {
  headers: {
    'Content-Type': 'text/event-stream',
    'Cache-Control': 'no-cache',
    'Connection': 'keep-alive',
  },
});
```

## Frontend Implementation

**Location:** `components/chat/useStreamingChat.ts`

**Key Code:**
```typescript
const reader = response.body!.getReader();
const decoder = new TextDecoder();
let buffer = '';

while (true) {
  const { done, value } = await reader.read();
  if (done) break;

  buffer += decoder.decode(value, { stream: true });
  const lines = buffer.split('\n');
  buffer = lines.pop() || '';

  for (const line of lines) {
    if (!line.startsWith('data: ')) continue;
    const dataStr = line.substring(6);

    if (dataStr === '[DONE]') {
      // Streaming complete
      return { success: true, conversationId, finalContent };
    }

    const event: StreamEvent = JSON.parse(dataStr);

    if (event.type === 'token') {
      // Accumulate with batching (16ms)
      pendingTokensRef.current += event.content;
      if (!batchTimerRef.current) {
        batchTimerRef.current = setTimeout(() => {
          startTransition(() => {
            setStreamingContent(prev => prev + pendingTokensRef.current);
            pendingTokensRef.current = '';
          });
        }, 16);
      }
    }

    if (event.type === 'status') {
      setStatus(event.message);
    }

    if (event.type === 'conversationId') {
      receivedConversationId = event.conversationId;
    }

    if (event.type === 'error') {
      return { success: false, error: event.message };
    }
  }
}
```

## Performance Optimizations

### 1. Token Batching (16ms window)

**Problem:** Streaming sends tokens very frequently (100-200 tokens/sec). Each token triggered a React state update, causing 100-200 re-renders per second.

**Solution:** Accumulate tokens for 16ms (1 frame at 60fps) before updating React state. This reduces re-renders by 60x.

**Implementation:**
```typescript
// Instead of immediate update:
setStreamingContent(prev => prev + token); // ❌ Causes re-render

// Batch for 16ms:
pendingTokensRef.current += token; // ✅ Accumulate in ref
setTimeout(() => {
  setStreamingContent(prev => prev + pendingTokensRef.current);
  pendingTokensRef.current = '';
}, 16);
```

### 2. useTransition for Non-Urgent Updates

**Problem:** Streaming updates could block user interactions (typing, clicking).

**Solution:** Mark streaming updates as non-urgent using React 18 `useTransition`. This allows React to prioritize user input over token rendering.

**Implementation:**
```typescript
const [, startTransition] = useTransition();

startTransition(() => {
  setStreamingContent(prev => prev + pendingTokensRef.current);
});
```

### 3. React.memo on MessageBubble

**Problem:** Every token update re-rendered ALL messages in the conversation.

**Solution:** Wrap MessageBubble in `React.memo` to prevent re-render of old messages.

**Implementation:**
```typescript
export const MessageBubble = memo(function MessageBubble({ message, streaming }) {
  // Only re-renders if message.content or streaming prop changes
});
```

### Combined Impact

- **Before:** 100-200 re-renders/sec × N messages = 500-1000 re-renders/sec (for 5 messages)
- **After:** 60 re-renders/sec × 1 message (streaming) = 60 re-renders/sec
- **Improvement:** ~90% reduction in re-renders, ensuring 60fps smooth scrolling

## Error Handling

### Connection Drop Mid-Stream

**Scenario:** Network connection lost while streaming

**Handling:**
- Frontend `fetch` throws error
- `useStreamingChat` catches error, checks `error.name === 'AbortError'`
- Display reconnection message (future: auto-retry)

### OpenAI API Error During Streaming

**Scenario:** OpenAI rate limit, API error, timeout

**Handling:**
- Backend catches error in `try/catch`
- Emits error event: `{"type":"error","message":"..."}`
- Frontend displays error, halts streaming

### Tool Execution Error During Streaming

**Scenario:** `read_file` fails (file not found), `execute_workflow` fails (invalid YAML)

**Handling:**
- Backend catches error in tool execution
- Returns error result: `{success: false, error: "..."}`
- LLM receives error as tool result, can retry or inform user
- Streaming continues (error is part of conversation context)

### Timeout Errors

**Scenario:** Streaming exceeds 60 seconds

**Handling:**
- Backend eventually times out (platform-dependent)
- Frontend `fetch` timeout (if configured)
- Display timeout error, allow user to retry

## Testing Considerations

### Unit Tests

- Parse SSE events correctly (handle split chunks, incomplete lines)
- Token accumulation works (verify final content matches all tokens)
- Status events update status state
- Error events halt streaming
- [DONE] event completes stream

### Integration Tests

- Streaming + tool calls: Verify pause/resume works
- Streaming + workflows: Verify lazy-loading works during streaming
- Streaming + file attachments: Verify context injection preserved
- Multi-turn conversations: Verify conversationId propagates correctly

### Regression Tests (CRITICAL)

- All existing tests must pass (no regressions)
- Test complex workflows (2-3 from bundles/bmm/agents)
- Test file operations (read_file, write_file, list_files)
- Test multi-turn conversations

### Performance Tests

- 60fps smooth scrolling during streaming
- Memory usage stable during long responses
- No layout thrashing from rapid token updates
- Browser DevTools performance profiling

## References

- **Story:** `/docs/stories/story-6.8.md`
- **Architecture:** `/docs/epic-6-architecture.md` (Feature 5: Streaming Responses, lines 702-917)
- **Backend Implementation:** `/app/api/chat/route.ts`
- **Frontend Hook:** `/components/chat/useStreamingChat.ts`
- **OpenAI Streaming Docs:** https://platform.openai.com/docs/api-reference/streaming
- **MDN SSE Spec:** https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events

---

**Document Version:** 1.0
**Last Updated:** 2025-10-09
**Author:** Amelia (Dev Implementation Agent)
**Story:** 6.8 - Visual Response Streaming Display
